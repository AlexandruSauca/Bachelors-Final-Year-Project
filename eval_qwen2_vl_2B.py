from unstructured.partition.pdf import partition_pdf
import base64
from IPython.display import Image, display, Markdown
from transformers import AutoProcessor, BitsAndBytesConfig, Qwen2VLForConditionalGeneration
import torch
from qwen_vl_utils import process_vision_info
from google import genai
from dotenv import load_dotenv
import enum

load_dotenv()

client = genai.Client(api_key="AIzaSyB5F3YWV_wyvuQFhZ3b-jnbzzql02T_ZW0")

prompt_images = """Describe the content of the image in detail.
For context,the image is part of a research paper explaining the transformers
architecture. Be specific about graphs, such as bar plots."""

prompt_text = """You are an assistant tasked with summarizing tables and text.
Give a concise summary of the table or text.

Respond only with the summary, no additionnal comment.
Do not start your message by saying "Here is a summary" or anything like that.
Just give the summary as it is.

Table or text chunk: {element}"""

SUMMARY_PROMPT = """\
# Instruction
You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.
We will provide you with the user input and an AI-generated responses.
You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.
You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric.

# Evaluation
## Metric Definition
You will be assessing summarization quality, which measures the overall ability to summarize text. Pay special attention to length constraints, such as in X words or in Y sentences. The instruction for performing a summarization task and the context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.

## Criteria
Instruction following: The response demonstrates a clear understanding of the summarization task instructions, satisfying all of the instruction's requirements.
Groundedness: The response contains information included only in the context. The response does not reference any outside information.
Conciseness: The response summarizes the relevant details in the original text without a significant loss in key information without being too verbose or terse.
Fluency: The response is well-organized and easy to read.

## Rating Rubric
5: (Very good). The summary follows instructions, is grounded, is concise, and fluent.
4: (Good). The summary follows instructions, is grounded, concise, and fluent.
3: (Ok). The summary mostly follows instructions, is grounded, but is not very concise and is not fluent.
2: (Bad). The summary is grounded, but does not follow the instructions.
1: (Very bad). The summary is not grounded.

## Evaluation Steps
STEP 1: Assess the response in aspects of instruction following, groundedness, conciseness, and verbosity according to the criteria.
STEP 2: Score based on the rubric.

# User Inputs and AI-generated Response
## User Inputs

### Prompt
{prompt}

## AI-generated Response
{response}
"""

class SummaryRating(enum.Enum):
  VERY_GOOD = '5'
  GOOD = '4'
  OK = '3'
  BAD = '2'
  VERY_BAD = '1'

def chunk_pdf(file_path):
    chunks = partition_pdf(
    filename=file_path, # path to the PDF file to be partitioned
    infer_table_structure=True, #enables automatic detection and structuring of tables within the document
    strategy="hi_res", #most accurate, but potentially slowest and most resource-intensive, strategy for analyzing a document's layout and content
    extract_image_block_types=["Image", "Tables"], #extracts images and tables locally
    extract_image_block_output_dir="images", #saves images and tables to the specified directory
    extract_image_block_to_payload=True, #metadata with base64
    chunking_strategy="by_title",
    max_characters=10000,
    combine_text_under_n_chars=2000,
    new_after_n_chars=6000,
    )
    return chunks

def get_text(chunks):
    texts = []
    for chunk in chunks:
        if 'CompositeElement' in str(type(chunk)):
            texts.append(chunk.text)
    return texts

def get_tables(chunks):
    tables = []
    for chunk in chunks:
        if 'Table' in str(type(chunk)):
            tables.append(chunk)
    return tables

def get_images_base64(chunks):
    image_64=[]
    for chunk in chunks:
        if 'CompositeElement' in str(type(chunk)):
            chunk_el =chunk.metadata.orig_elements
            for el in chunk_el:
                if 'Image' in str(type(el)):
                    image_64.append(el.metadata.image_base64)
    return image_64

def display_image_base64(base64_code):
  image_data= base64.b64decode(base64_code) #decode base64 to binary
  display(Image(data=image_data))

def proccess_images_base64(images_base64, processor, model, prompt = prompt_images, max_tokens=128):
    """
    Generates a text description for a single base64-encoded image using Qwen-VL.

    Args:
        base64_image (str): The raw base64 encoded image string (without the 'data:image/jpeg;base64,' prefix).
        model: The loaded Qwen2VLForConditionalGeneration model.
        processor: The loaded AutoProcessor.
        prompt_text (str): The text prompt to guide the description.
        max_tokens (int): The maximum number of new tokens to generate.

    Returns:
        str: The generated text description.
    """
    message_images = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "data:image;base64," + images_base64},
            {"type": "text", "text": prompt_images},
        ],
    }
    ]
    text = processor.apply_chat_template(
    message_images, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(message_images)
    inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
    )
    inputs = inputs.to("cuda")

    # Inference: Generation of the output
    generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)
    generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
    return output_text[0]

def proccess_text(text_chunk, processor, model, prompt=prompt_text, max_tokens=128):
    final_prompt = prompt.format(element=text_chunk)
    messages = [
    {
        "role": "user",
        "content": [
            {"type": "text", "text": final_prompt} 
        ],
    }
    ]

    text = processor.apply_chat_template(
    messages, 
    tokenize=False, 
    add_generation_prompt=True
    )

    inputs = processor(
    text=[text],
    images=None,  # Explicitly state there are no images
    videos=None,
    padding=True,
    return_tensors="pt",
    ).to("cuda")

    generated_ids = model.generate(**inputs, max_new_tokens=150) # Give it 150 tokens for a summary
    generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )

    return output_text[0] 

def evaluate_summary(prompt, ai_response):
    """Evaluate the generated summary against the prompt used."""
    chat = client.chats.create(model='gemini-2.5-pro')
    response = chat.send_message(
      message=SUMMARY_PROMPT.format(prompt=prompt, response=ai_response)
    )
    verbose_eval = response.text
    
    return verbose_eval

def summarize_text_gemini(prompt):
    chat = client.chats.create(model='gemini-2.5-pro')
    response = chat.send_message(
      message=prompt
    )
    summary = response.text
    
    return summary

def main():
    file = "./Examples/1706.03762v7.pdf"
    chunks = chunk_pdf(file)
    print(f"Total Chunks: {len(chunks)}")

    images_base64 = get_images_base64(chunks)
    print(f"Total Images: {len(images_base64)}")

    model_id = "./qwen2_vl_model"
    quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
    )
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        model_id,
        dtype=torch.float16,
        device_map="auto",
        quantization_config = quant_config
    )
    min_pixels = 256*28*28
    max_pixels = 488*28*28
    processor = AutoProcessor.from_pretrained(
        model_id,
        min_pixels = min_pixels,
        max_pixels = max_pixels
    )
    texts = get_text(chunks)[1]
    #print(proccess_images_base64(images_base64[0], processor, model, prompt=prompt_images, max_tokens=128))
    #print(proccess_text(text_chunk=texts, processor=processor, model=model, prompt=prompt_text, max_tokens=128))

    model_response = proccess_text(text_chunk=texts, processor=processor, model=model, prompt=prompt_text, max_tokens=128)
    print("Qwen2-VL Summary:")
    print(model_response)
    text_eval = evaluate_summary(prompt=prompt_text, ai_response=model_response)
    print(text_eval) #bad summary

    # summary_gemini =summarize_text_gemini(prompt= texts)
    # print("Gemini 2.5 Pro Summary:")
    # print(summary_gemini)
    

if __name__ == "__main__":
    main()